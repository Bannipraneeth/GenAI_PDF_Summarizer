import os
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from dotenv import load_dotenv

# Load environment variables from API.env file
load_dotenv('API.env')

def summarize_pdf(pdf_file_path, custom_prompt_text, chain_type, chunk_size, chunk_overlap):
    """
    Summarizes a PDF using a user-provided prompt, chain type, and chunk parameters with Gemini.
    """
    # 1. Instantiate LLM model
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        st.error("GEMINI_API_KEY not found. Please set it in your API.env file.")
        return None

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        temperature=0.3,
        google_api_key=api_key
    )

    # 2. Load and split the PDF using dynamic chunk parameters
    loader = PyPDFLoader(pdf_file_path)
    docs_chunks = loader.load_and_split(
        text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    )

    # 3. Create and run the summarization chain based on the selected type
    if chain_type == "stuff":
        # Create the prompt from the user's template for the 'stuff' method
        prompt_template = custom_prompt_text + """

        {text}

        """
        prompt = PromptTemplate.from_template(prompt_template)
        
        # Create the 'stuff' summarization chain
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt)
    
    elif chain_type == "map_reduce":
        # Define prompts for the 'map_reduce' method
        # The map prompt summarizes each chunk individually
        map_prompt_template = """
        Write a concise summary of the following text chunk:
        {text}
        """
        map_prompt = PromptTemplate.from_template(map_prompt_template)

        # The combine prompt uses the user's custom instructions to combine the chunk summaries
        combine_prompt_template = custom_prompt_text + """

        {text}
        """
        combine_prompt = PromptTemplate.from_template(combine_prompt_template)
        
        # Create the 'map_reduce' summarization chain
        chain = load_summarize_chain(
            llm,
            chain_type="map_reduce",
            map_prompt=map_prompt,
            combine_prompt=combine_prompt,
            verbose=True
        )

    # Invoke the chain with the document chunks
    result = chain.invoke({"input_documents": docs_chunks})
    
    return result['output_text']

def main():
    st.set_page_config(page_title="Advanced PDF Summarizer", page_icon="‚úçÔ∏è", layout="wide")
    
    st.title("‚úçÔ∏è Advanced PDF Summarizer with Gemini")
    st.markdown("This app allows you to upload a PDF file, provide a custom instruction (a prompt), and get a tailored summary generated by Google's Gemini model.")

    # --- Sidebar for Controls ---
    with st.sidebar:
        st.header("‚öôÔ∏è Controls")
        
        # Dropdown to select the summarization method
        chain_type = st.selectbox(
            "**Choose Summarization Method**",
            ("stuff", "map_reduce"),
            help="**Stuff**: Faster, but may fail on very large documents. **MapReduce**: Slower, but handles large documents by summarizing chunks first."
        )

        # Warning for the map_reduce method
        if chain_type == "map_reduce":
            st.warning("The 'MapReduce' method can take significantly longer to process.")

        # Sliders for chunk size and overlap
        chunk_size = st.slider(
            "**Chunk Size**",
            min_value=500,
            max_value=20000,
            value=4000,
            step=500,
            help="The maximum number of characters in each text chunk."
        )
        chunk_overlap = st.slider(
            "**Chunk Overlap**",
            min_value=0,
            max_value=5000,
            value=200,
            step=50,
            help="The number of characters to overlap between adjacent chunks."
        )

    # --- Main App Area ---
    uploaded_file = st.file_uploader("**1. Upload your PDF file**", type="pdf")

    if uploaded_file is not None:
        # Save the uploaded file to a temporary location
        temp_file_path = os.path.join(".", "temp_uploaded_file.pdf")
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.info(f"‚úÖ Successfully uploaded `{uploaded_file.name}`")

        custom_prompt = st.text_area("**2. Enter your custom prompt**", height=150,
                                     placeholder="For example: 'Summarize the key findings of this research paper for a non-technical audience in five bullet points.'")

        if st.button("**Generate Summary**", type="primary"):
            if not custom_prompt:
                st.warning("Please enter a prompt to guide the summary.")
            else:
                with st.spinner("üß† Gemini is thinking... This might take a moment."):
                    try:
                        # Call the summarize function with the new parameters from the sidebar
                        summary = summarize_pdf(temp_file_path, custom_prompt, chain_type, chunk_size, chunk_overlap)
                        if summary:
                            st.subheader("Your Custom Summary")
                            st.success(summary)
                    except Exception as e:
                        st.error(f"An error occurred: {e}")
        
        # Clean up the temporary file after processing
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

if __name__ == "__main__":
    main()
